{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nice-only.ipynb","provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyNRHIp1+XW2SsCcU8hGaqsD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"eS6NbxgXuPHq"},"source":["import numpy as np\n","import torch\n","import torchvision as tvis\n","import torch.nn as nn\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVRHuMWhuTPL"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZ3qlD1duWvp"},"source":["\n","def get_train_valid_loader(data_dir,\n","                           batch_size,\n","                           random_seed,\n","                           valid_size=0.02,\n","                           shuffle=True,\n","                           num_workers=2,\n","                           pin_memory=True):\n","\n","    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n","    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n","\n","\n","    # define transforms\n","    valid_transform = transforms.Compose([\n","            transforms.ToTensor()        ])\n","    train_transform = transforms.Compose([\n","            transforms.ToTensor()\n","        ])\n","\n","    # load the dataset\n","    train_dataset = datasets.MNIST(root=data_dir, train=True, \n","                download=False, transform=train_transform)\n","\n","    valid_dataset = datasets.MNIST(root=data_dir, train=True, \n","                download=False, transform=valid_transform)\n","\n","    num_train = len(train_dataset)\n","    indices = list(range(num_train))\n","    split = int(np.floor(valid_size * num_train))\n","\n","    if shuffle == True:\n","        np.random.seed(random_seed)\n","        np.random.shuffle(indices)\n","\n","    train_idx, valid_idx = indices[split:], indices[:split]\n","\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","    train_loader = torch.utils.data.DataLoader(train_dataset, \n","                    batch_size=batch_size, sampler=train_sampler, \n","                    num_workers=num_workers, pin_memory=pin_memory)\n","\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset, \n","                    batch_size=batch_size, sampler=valid_sampler, \n","                    num_workers=num_workers, pin_memory=pin_memory)\n","\n","    return (train_loader, valid_loader)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnqgjWDDucd_"},"source":["\"\"\"Utility classes for NICE.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","\n","\"\"\"Additive coupling layer.\n","\"\"\"\n","class Coupling(nn.Module):\n","    def __init__(self, in_out_dim, mid_dim, hidden, mask_config):\n","        \"\"\"Initialize a coupling layer.\n","        Args:\n","            in_out_dim: input/output dimensions.\n","            mid_dim: number of units in a hidden layer.\n","            hidden: number of hidden layers.\n","            mask_config: 1 if transform odd units, 0 if transform even units.\n","        \"\"\"\n","        super(Coupling, self).__init__()\n","        self.mask_config = mask_config\n","\n","        self.in_block = nn.Sequential(\n","            nn.Linear(in_out_dim//2, mid_dim),\n","            nn.ReLU())\n","        self.mid_block = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(mid_dim, mid_dim),\n","                nn.ReLU()) for _ in range(hidden - 1)])\n","        self.out_block = nn.Linear(mid_dim, in_out_dim//2)\n","\n","    def forward(self, x, reverse=False):\n","        \"\"\"Forward pass.\n","        Args:\n","            x: input tensor.\n","            reverse: True in inference mode, False in sampling mode.\n","        Returns:\n","            transformed tensor.\n","        \"\"\"\n","        [B, W] = list(x.size())\n","        x = x.reshape((B, W//2, 2))\n","        if self.mask_config:\n","            on, off = x[:, :, 0], x[:, :, 1]\n","        else:\n","            off, on = x[:, :, 0], x[:, :, 1]\n","\n","        off_ = self.in_block(off)\n","        for i in range(len(self.mid_block)):\n","            off_ = self.mid_block[i](off_)\n","        shift = self.out_block(off_)\n","        if reverse:\n","            on = on - shift\n","        else:\n","            on = on + shift\n","\n","        if self.mask_config:\n","            x = torch.stack((on, off), dim=2)\n","        else:\n","            x = torch.stack((off, on), dim=2)\n","        return x.reshape((B, W))\n","\n","\"\"\"Log-scaling layer.\n","\"\"\"\n","class Scaling(nn.Module):\n","    def __init__(self, dim):\n","        \"\"\"Initialize a (log-)scaling layer.\n","        Args:\n","            dim: input/output dimensions.\n","        \"\"\"\n","        super(Scaling, self).__init__()\n","        self.scale = nn.Parameter(\n","            torch.zeros((1, dim)), requires_grad=True)\n","\n","    def forward(self, x, reverse=False):\n","        \"\"\"Forward pass.\n","        Args:\n","            x: input tensor.\n","            reverse: True in inference mode, False in sampling mode.\n","        Returns:\n","            transformed tensor and log-determinant of Jacobian.\n","        \"\"\"\n","        log_det_J = torch.sum(self.scale)\n","        if reverse:\n","            x = x * torch.exp(-self.scale)\n","        else:\n","            x = x * torch.exp(self.scale)\n","        return x, log_det_J\n","\n","\"\"\"NICE main model.\n","\"\"\"\n","class NICE(nn.Module):\n","    def __init__(self, prior, coupling, \n","        in_out_dim, mid_dim, hidden, mask_config):\n","        \"\"\"Initialize a NICE.\n","        Args:\n","            prior: prior distribution over latent space Z.\n","            coupling: number of coupling layers.\n","            in_out_dim: input/output dimensions.\n","            mid_dim: number of units in a hidden layer.\n","            hidden: number of hidden layers.\n","            mask_config: 1 if transform odd units, 0 if transform even units.\n","        \"\"\"\n","        super(NICE, self).__init__()\n","        self.prior = prior\n","        self.in_out_dim = in_out_dim\n","\n","        self.coupling = nn.ModuleList([\n","            Coupling(in_out_dim=in_out_dim, \n","                     mid_dim=mid_dim, \n","                     hidden=hidden, \n","                     mask_config=(mask_config+i)%2) \\\n","            for i in range(coupling)])\n","        self.scaling = Scaling(in_out_dim)\n","\n","    def g(self, z):\n","        \"\"\"Transformation g: Z -> X (inverse of f).\n","        Args:\n","            z: tensor in latent space Z.\n","        Returns:\n","            transformed tensor in data space X.\n","        \"\"\"\n","        x, _ = self.scaling(z, reverse=True)\n","        for i in reversed(range(len(self.coupling))):\n","            x = self.coupling[i](x, reverse=True)\n","        return x\n","\n","    def f(self, x):\n","        \"\"\"Transformation f: X -> Z (inverse of g).\n","        Args:\n","            x: tensor in data space X.\n","        Returns:\n","            transformed tensor in latent space Z.\n","        \"\"\"\n","        for i in range(len(self.coupling)):\n","            x = self.coupling[i](x)\n","        return self.scaling(x)\n","\n","    def log_prob(self, x):\n","        \"\"\"Computes data log-likelihood.\n","        (See Section 3.3 in the NICE paper.)\n","        Args:\n","            x: input minibatch.\n","        Returns:\n","            log-likelihood of input.\n","        \"\"\"\n","        z, log_det_J = self.f(x)\n","        log_ll = torch.sum(self.prior.log_prob(z), dim=1)\n","        return log_ll + log_det_J\n","\n","    def sample(self, size):\n","        \"\"\"Generates samples.\n","        Args:\n","            size: number of samples to generate.\n","        Returns:\n","            samples from the data space X.\n","        \"\"\"\n","        z = self.prior.sample((size, self.in_out_dim)).cuda()\n","        return self.g(z)\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass.\n","        Args:\n","            x: input minibatch.\n","        Returns:\n","            log-likelihood of input.\n","        \"\"\"\n","        return self.log_prob(x)\n","\n","import torch\n","import torch.nn.functional as F\n","\n","def dequantize(x, dataset):\n","    '''Dequantize data.\n","    Add noise sampled from Uniform(0, 1) to each pixel (in [0, 255]).\n","    Args:\n","        x: input tensor.\n","        reverse: True in inference mode, False in training mode.\n","    Returns:\n","        dequantized data.\n","    '''\n","    noise = torch.distributions.Uniform(0., 1.).sample(x.size())\n","    return (x * 255. + noise) / 256.\n","\n","def prepare_data(x, dataset, zca=None, mean=None, reverse=False):\n","    \"\"\"Prepares data for NICE.\n","    In training mode, flatten and dequantize the input.\n","    In inference mode, reshape tensor into image size.\n","    Args:\n","        x: input minibatch.\n","        dataset: name of dataset.\n","        zca: ZCA whitening transformation matrix.\n","        mean: center of original dataset.\n","        reverse: True if in inference mode, False if in training mode.\n","    Returns:\n","        transformed data.\n","    \"\"\"\n","    if reverse:\n","        assert len(list(x.size())) == 2\n","        [B, W] = list(x.size())\n","\n","        if dataset in ['mnist', 'fashion-mnist']:\n","            assert W == 1 * 28 * 28\n","            x += mean\n","            x = x.reshape((B, 1, 28, 28))\n","        elif dataset in ['svhn', 'cifar10']:\n","            assert W == 3 * 32 * 32\n","            x = torch.matmul(x, zca.inverse()) + mean\n","            x = x.reshape((B, 3, 32, 32))\n","    else:\n","        assert len(list(x.size())) == 4\n","        [B, C, H, W] = list(x.size())\n","\n","        if dataset in ['mnist', 'fashion-mnist']:\n","            assert [C, H, W] == [1, 28, 28]\n","        elif dataset in ['svhn', 'cifar10']:\n","            assert [C, H, W] == [3, 32, 32]\n","\n","        x = dequantize(x, dataset)\n","        x = x.reshape((B, C*H*W))\n","\n","        if dataset in ['mnist', 'fashion-mnist']:\n","            x -= mean\n","        elif dataset in ['svhn', 'cifar10']:\n","            x = torch.matmul((x - mean), zca)\n","    return x\n","\n","\"\"\"Standard logistic distribution.\n","\"\"\"\n","class StandardLogistic(torch.distributions.Distribution):\n","    def __init__(self):\n","        super(StandardLogistic, self).__init__()\n","\n","    def log_prob(self, x):\n","        \"\"\"Computes data log-likelihood.\n","        Args:\n","            x: input tensor.\n","        Returns:\n","            log-likelihood.\n","        \"\"\"\n","        return -(F.softplus(x) + F.softplus(-x))\n","\n","    def sample(self, size):\n","        \"\"\"Samples from the distribution.\n","        Args:\n","            size: number of samples to generate.\n","        Returns:\n","            samples.\n","        \"\"\"\n","        z = torch.distributions.Uniform(0., 1.).sample(size).cuda()\n","        return torch.log(z) - torch.log(1. - z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5LefUgIuoh8"},"source":["import torch, torchvision\n","import numpy as np\n","\n","\n","\n","device = torch.device(\"cuda:0\")\n","data_dir = \"./gdrive/My Drive/coding_projects/data\"\n","\n","# model hyperparameters\n","dataset = \"mnist\"\n","batch_size = 200\n","latent = 512\n","max_iter = 5000\n","decay = 0.999\n","momentum=0.9\n","lr = 1e-3\n","sample_size = 64\n","coupling = 4\n","mask_config = 1\n","latent = 'normal'\n","\n","\n","zca = None\n","mean = None\n","mean = torch.load(\"./gdrive/My Drive/coding_projects/data/MNIST/mnist_mean.pt\")\n","(full_dim, mid_dim, hidden) = (1 * 28 * 28, 1000, 5)\n","\n","\n","if latent == 'normal':\n","    prior = torch.distributions.Normal(\n","        torch.tensor(0.).to(device), torch.tensor(1.).to(device))\n","elif latent == 'logistic':\n","    prior = StandardLogistic()\n","\n","trainloader, vl_load = get_train_valid_loader(data_dir,batch_size=batch_size, num_workers=2, random_seed=0)\n","\n","flow = NICE(prior=prior, \n","            coupling=coupling, \n","            in_out_dim=full_dim, \n","            mid_dim=mid_dim, \n","            hidden=hidden, \n","            mask_config=mask_config).to(device)\n","optimizer = torch.optim.Adam(\n","    flow.parameters(), lr=lr, betas=(momentum, decay), eps=1e-4)\n","\n","total_iter = 0\n","train = True\n","running_loss = 0\n","\n","while train:\n","    for _, data in enumerate(trainloader, 1):\n","        flow.train()    # set to training mode\n","        if total_iter == max_iter:\n","            train = False\n","            break\n","\n","        total_iter += 1\n","        optimizer.zero_grad()    # clear gradient tensors\n","\n","        inputs, _ = data\n","        inputs = prepare_data(\n","            inputs, dataset, zca=zca, mean=mean).to(device)\n","\n","        # log-likelihood of input minibatch\n","        loss = -flow(inputs).mean()\n","        running_loss += float(loss)\n","\n","        # backprop and update parameters\n","        loss.backward()\n","        optimizer.step()\n","\n","        if total_iter % 1000 == 0:\n","            mean_loss = running_loss / 1000\n","            bit_per_dim = (mean_loss + np.log(256.) * full_dim) \\\n","                        / (full_dim * np.log(2.))\n","            print('iter %s:' % total_iter, \n","                'loss = %.3f' % mean_loss, \n","                'bits/dim = %.3f' % bit_per_dim)\n","            running_loss = 0.0\n","\n","            flow.eval()        # set to inference mode\n","with torch.no_grad():\n","    z, _ = flow.f(inputs)\n","    reconst = flow.g(z).cpu()\n","    reconst = prepare_data(\n","        reconst, dataset, zca=zca, mean=mean, reverse=True)\n","    samples = flow.sample(sample_size).cpu()\n","    samples = prepare_data(\n","        samples, dataset, zca=zca, mean=mean, reverse=True)\n","                \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97US0kjKv1S0"},"source":["import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vk-tNyEPx1BY"},"source":["index_ = 0\n","plt.figure()\n","plt.imshow(inputs[index_].cpu().detach().numpy().reshape(28,28))\n","plt.figure()\n","plt.imshow(reconst[index_].cpu().detach().numpy().reshape(28,28))\n","plt.figure()\n","plt.imshow(samples[index_].cpu().detach().numpy().reshape(28,28))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHdGWe-ax2LU"},"source":["flow.parameters()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GiRWcTgF5gON"},"source":[""],"execution_count":null,"outputs":[]}]}